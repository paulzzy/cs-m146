{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40nOq82Ha1zB"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asomlC24dVik"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_NuDNh9rUos"
      },
      "outputs": [],
      "source": [
        "# To add your own Drive Run this cell.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWPBfJLfrkhJ"
      },
      "outputs": [],
      "source": [
        "# Please append your own directory after â€˜/content/drive/My Drive/'\n",
        "# where you have nutil.py and adult_subsample.csv\n",
        "### ========== TODO : START ========== ###\n",
        "#sys.path += ['/content/drive/My Drive/cm146-2024-homeworks/PS1'] # example path\n",
        "sys.path += ['/content/drive/'] # your path\n",
        "\n",
        "### ========== TODO : END ========== ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxKg7xF1r82H"
      },
      "outputs": [],
      "source": [
        "from nutil import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55EocyDPsWIb"
      },
      "outputs": [],
      "source": [
        "# Use only the provided packages!\n",
        "import math\n",
        "import csv\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxpkWVRetDgo"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Immutable classes\n",
        "######################################################################\n",
        "\n",
        "class Classifier(object) :\n",
        "    \"\"\"\n",
        "    Classifier interface.\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, X):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class MajorityVoteClassifier(Classifier) :\n",
        "\n",
        "    def __init__(self) :\n",
        "        \"\"\"\n",
        "        A classifier that always predicts the majority class.\n",
        "\n",
        "        Attributes\n",
        "        --------------------\n",
        "            prediction_ -- majority class\n",
        "        \"\"\"\n",
        "        self.prediction_ = None\n",
        "\n",
        "    def fit(self, X, y) :\n",
        "        \"\"\"\n",
        "        Build a majority vote classifier from the training set (X, y).\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "            X    -- numpy array of shape (n,d), samples\n",
        "            y    -- numpy array of shape (n,), target classes\n",
        "\n",
        "        Returns\n",
        "        --------------------\n",
        "            self -- an instance of self\n",
        "        \"\"\"\n",
        "        majority_val = Counter(y).most_common(1)[0][0]\n",
        "        self.prediction_ = majority_val\n",
        "        return self\n",
        "\n",
        "    def predict(self, X) :\n",
        "        \"\"\"\n",
        "        Predict class values.\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "            X    -- numpy array of shape (n,d), samples\n",
        "\n",
        "        Returns\n",
        "        --------------------\n",
        "            y    -- numpy array of shape (n,), predicted classes\n",
        "        \"\"\"\n",
        "        if self.prediction_ is None :\n",
        "            raise Exception(\"Classifier not initialized. Perform a fit first.\")\n",
        "\n",
        "        n,d = X.shape\n",
        "        y = [self.prediction_] * n\n",
        "        return y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yefbwe8EvH-V"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Mutable classes\n",
        "######################################################################\n",
        "\n",
        "class RandomClassifier(Classifier) :\n",
        "\n",
        "    def __init__(self) :\n",
        "        \"\"\"\n",
        "        A classifier that predicts according to the distribution of the classes.\n",
        "\n",
        "        Attributes\n",
        "        --------------------\n",
        "            probabilities_ -- class distribution dict (key = class, val = probability of class)\n",
        "        \"\"\"\n",
        "        self.probabilities_ = None\n",
        "\n",
        "    def fit(self, X, y) :\n",
        "        \"\"\"\n",
        "        Build a random classifier from the training set (X, y).\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "            X    -- numpy array of shape (n,d), samples\n",
        "            y    -- numpy array of shape (n,), target classes\n",
        "\n",
        "        Returns\n",
        "        --------------------\n",
        "            self -- an instance of self\n",
        "        \"\"\"\n",
        "\n",
        "        ### ========== TODO : START ========== ###\n",
        "        # part b: set self.probabilities_ according to the training set\n",
        "\n",
        "\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, seed=1234) :\n",
        "        \"\"\"\n",
        "        Predict class values.\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "            X    -- numpy array of shape (n,d), samples\n",
        "            seed -- integer, random seed\n",
        "\n",
        "        Returns\n",
        "        --------------------\n",
        "            y    -- numpy array of shape (n,), predicted classes\n",
        "        \"\"\"\n",
        "        if self.probabilities_ is None :\n",
        "            raise Exception(\"Classifier not initialized. Perform a fit first.\")\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        ### ========== TODO : START ========== ###\n",
        "        # part b: predict the class for each test example\n",
        "        # hint: use np.random.choice (be careful of the parameters)\n",
        "\n",
        "\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m9qVosFwbAK"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Immutable functions\n",
        "######################################################################\n",
        "\n",
        "def plot_histograms(X, y, Xnames, yname) :\n",
        "    n,d = X.shape  # n = number of examples, d =  number of features\n",
        "    fig = plt.figure(figsize=(20,15))\n",
        "    ncol = 3\n",
        "    nrow = d // ncol + 1\n",
        "    for i in range(d) :\n",
        "        fig.add_subplot (nrow,ncol,i+1)\n",
        "        data, bins, align, labels = plot_histogram(X[:,i], y, Xname=Xnames[i], yname=yname, show = False)\n",
        "        n, bins, patches = plt.hist(data, bins=bins, align=align, alpha=0.5, label=labels)\n",
        "        plt.xlabel(Xnames[i])\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend() #plt.legend(loc='upper left')\n",
        "\n",
        "    plt.savefig ('histograms.pdf')\n",
        "\n",
        "\n",
        "def plot_histogram(X, y, Xname, yname, show = True) :\n",
        "    \"\"\"\n",
        "    Plots histogram of values in X grouped by y.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        X     -- numpy array of shape (n,d), feature values\n",
        "        y     -- numpy array of shape (n,), target classes\n",
        "        Xname -- string, name of feature\n",
        "        yname -- string, name of target\n",
        "    \"\"\"\n",
        "\n",
        "    # set up data for plotting\n",
        "    targets = sorted(set(y))\n",
        "    data = []; labels = []\n",
        "    for target in targets :\n",
        "        features = [X[i] for i in range(len(y)) if y[i] == target]\n",
        "        data.append(features)\n",
        "        labels.append('%s = %s' % (yname, target))\n",
        "\n",
        "    # set up histogram bins\n",
        "    features = set(X)\n",
        "    nfeatures = len(features)\n",
        "    test_range = list(range(int(math.floor(min(features))), int(math.ceil(max(features)))+1))\n",
        "    if nfeatures < 10 and sorted(features) == test_range:\n",
        "        bins = test_range + [test_range[-1] + 1] # add last bin\n",
        "        align = 'left'\n",
        "    else :\n",
        "        bins = 10\n",
        "        align = 'mid'\n",
        "\n",
        "    # plot\n",
        "    if show == True:\n",
        "        plt.figure()\n",
        "        n, bins, patches = plt.hist(data, bins=bins, align=align, alpha=0.5, label=labels)\n",
        "        plt.xlabel(Xname)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend() #plt.legend(loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    return data, bins, align, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z8oJrMgxc4_"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Mutable functions\n",
        "######################################################################\n",
        "\n",
        "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
        "    \"\"\"\n",
        "    Computes the classifier error over a random split of the data,\n",
        "    averaged over ntrials runs.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        clf         -- classifier\n",
        "        X           -- numpy array of shape (n,d), features values\n",
        "        y           -- numpy array of shape (n,), target classes\n",
        "        ntrials     -- integer, number of trials\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        train_error -- float, training error\n",
        "        test_error  -- float, test error\n",
        "        f1_score    -- float, test \"micro\" averaged f1 score\n",
        "    \"\"\"\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # compute cross-validation error using StratifiedShuffleSplit over ntrials\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "    return train_error, test_error, f1_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRkKf8DUxMdX"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Immutable functions\n",
        "######################################################################\n",
        "\n",
        "\n",
        "def write_predictions(y_pred, filename, yname=None) :\n",
        "    \"\"\"Write out predictions to csv file.\"\"\"\n",
        "    out = open(filename, 'wb')\n",
        "    f = csv.writer(out)\n",
        "    if yname :\n",
        "        f.writerow([yname])\n",
        "    f.writerows(list(zip(y_pred)))\n",
        "    out.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HdZPr0TsvYV"
      },
      "outputs": [],
      "source": [
        "\n",
        "######################################################################\n",
        "# main\n",
        "######################################################################\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "\n",
        "    # load adult_subsample dataset with correct file path\n",
        "    ### ========== TODO : START ========== ###\n",
        "    #data_file =  \"/content/drive/My Drive/cm146-2024-homeowrks/PS1/adult_subsample.csv\" # example path\n",
        "    data_file =  \"/content/drive/My Drive/YOUR/PATH/PS1/adult_subsample.csv\" # your path\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "    data = load_data(data_file, header=1, predict_col=-1)\n",
        "\n",
        "    X = data.X; Xnames = data.Xnames\n",
        "    y = data.y; yname = data.yname\n",
        "    n,d = X.shape  # n = number of examples, d =  number of features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #========================================\n",
        "    # part a: plot histograms of each feature\n",
        "    print('Plotting...')\n",
        "    plot_histograms (X, y, Xnames=Xnames, yname=yname)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part i: Preprocess X (e.g., normalize)\n",
        "    # (try this after finishing the sections below)\n",
        "\n",
        "    # X = ?\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #========================================\n",
        "    # train Majority Vote classifier on data\n",
        "    print('Classifying using Majority Vote...')\n",
        "    clf = MajorityVoteClassifier() # create MajorityVote classifier, which includes all model parameters\n",
        "    clf.fit(X, y)                  # fit training data using the classifier\n",
        "    y_pred = clf.predict(X)        # take the classifier and run it on the training data\n",
        "    train_error = 1 - metrics.accuracy_score(y, y_pred, normalize=True)\n",
        "    print('\\t-- training error: %.3f' % train_error)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part b: evaluate training error of Random classifier\n",
        "    print('Classifying using Random...')\n",
        "    clf = RandomClassifier ()\n",
        "\n",
        "    # print('\\t-- training error: %.3f' % train_error)\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part c: evaluate training error of Decision Tree classifier\n",
        "    print('Classifying using Decision Tree...')\n",
        "\n",
        "    # print('\\t-- training error: %.3f' % train_error)\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part d: evaluate training error of k-Nearest Neighbors classifier\n",
        "    # use k = 3, 5, 7 for n_neighbors\n",
        "    print('Classifying using k-Nearest Neighbors...')\n",
        "\n",
        "\n",
        "    # print the error for each k\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part e: use cross-validation to compute average training and test error of classifiers\n",
        "    print('Investigating various classifiers...')\n",
        "\n",
        "    # clf =\n",
        "\n",
        "    # summary = error(clf, X, y, ntrials=20)\n",
        "    # print(summary)\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part f: use 10-fold cross-validation to find the best value of k for k-Nearest Neighbors classifier\n",
        "    print('Finding the best k...')\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part g: investigate decision tree classifier with various depths\n",
        "    print('Investigating depths...')\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part h: investigate decision tree and k-Nearest Neighbors classifier with various training set sizes\n",
        "    # hint: use train_test_split (use random_state=0 for consistent results)\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
